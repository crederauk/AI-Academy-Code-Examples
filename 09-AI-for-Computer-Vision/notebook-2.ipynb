{"cells":[{"cell_type":"markdown","metadata":{"id":"SGiiAFP0cRvt"},"source":["# Object detection and motion detection using TensorFlow 2 and TensorFlow Lite\n","\n","In this notebook we will go through the process of configuring TensorFlow 2 to process an mp4 video frame-by-frame and perform object detection, paired with motion detection. \n","\n","This will involve:\n","\n","1. Cloning the TensorFlow Model Garden repo\n","2. Installing TensorFlow 2 to your Colab environment\n","3. Testing your environment with the pre-built TensorFlow Model Test's \n","4. Downloading a pre-trained TensorFlow Lite model\n","5. Running this model against an mp4 video\n","\n","If there are any issues with this notebook please contact james.milward@credera.co.uk\n","\n","\n","# Installation / Environment Configuration\n","\n","First, install the required libraries. We'll start off by getting the latest TensorFlow Models.\n","\n","**Note** the uninstall of Cython, it causes issues with TensorFlow packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13640,"status":"ok","timestamp":1725964283086,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"ptMlRJowcVgj","outputId":"8dca85e8-d027-45b6-dd8d-12022cc1f265"},"outputs":[],"source":["# Clone the tensorflow models repository from GitHub and remove Cython (causes issues with Colab)\n","!git clone --depth 1 https://github.com/tensorflow/models tensorflow-models\n","!pip uninstall Cython -y"]},{"cell_type":"markdown","metadata":{"id":"HCTrsRMRf1WI"},"source":["Next we'll install Tensorflow's object detection functionality by copying out the setup script and modifying it to support the latest version of TensorFlow that works with Colab (2.15.0)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725964283087,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"UhLvjwwWmaxF"},"outputs":[],"source":["# Copy setup files into models/research folder\n","%%bash\n","cd tensorflow-models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py ."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725964283087,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"FcYv9gMCn5uk"},"outputs":[],"source":["# Modify setup.py file to install the tf-models-official repository targeted at TF v2.15.0 (latest as of Aug 2024)\n","import re\n","with open('/content/tensorflow-models/research/object_detection/packages/tf2/setup.py') as f:\n","    s = f.read()\n","\n","with open('/content/tensorflow-models/research/setup.py', 'w') as f:\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('tf-models-official>=2.5.1',\n","               'tf-models-official==2.15.0', s)\n","    f.write(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":142486,"status":"ok","timestamp":1725964425570,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"t8Gb1eezig7P","outputId":"79c61cca-bb28-4329-e6e4-f5395fef9ca5"},"outputs":[],"source":["# Install TensorFlow specific version of pyyaml, install the object detection based on the modified setup.py and install TensorFlow\n","!pip install pyyaml==5.3\n","!pip install /content/tensorflow-models/research/\n","!pip install tensorflow==2.15.0"]},{"cell_type":"markdown","metadata":{"id":"LS7Yrlwjmkfi"},"source":["You may get errors or warnings with a modal window informing you that you need to restart. Ignore these errors, close the modal (don't restart) and move on.\n","\n","Next, lets install the model builder to make sure everything is working. The model builder test script runs through a few tests of the EfficientDet object detection model to ensure TensorFlow is working correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47241,"status":"ok","timestamp":1725964532592,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"t2hcATCzmk6a","outputId":"5cd8a1e7-5ffc-4d21-8c2e-73cf619068d7"},"outputs":[],"source":["# Run Model Bulider Test file, just to verify everything's working properly\n","!python /content/tensorflow-models/research/object_detection/builders/model_builder_tf2_test.py\n"]},{"cell_type":"markdown","metadata":{"id":"bfX71udqGJnD"},"source":["# Setup pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3967,"status":"ok","timestamp":1725964539741,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"wZs214oKGU2R","outputId":"99d14fb5-91b2-4e03-daf9-b2b6a7cfd6ba"},"outputs":[],"source":["# Clone the tensorflow models repository from GitHub and remove Cython (causes issues with Colab)\n","!git clone -b feat/colab-training --depth 1 https://github.com/Jimbwlah/tensorflow-ring-animal-detector"]},{"cell_type":"markdown","metadata":{"id":"70iilo3MSHSb"},"source":["# Test TensorFlow Lite Model\n","\n","We've now cloned a repo with a sample object detection model trained against animals. Let's get this setup in Python to execute object detection against a stock video.\n","\n","Firstly we change directory -"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1725964541723,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"udQJd15BUjrM","outputId":"a5c3ded8-b103-4ea2-8773-c914af6c5991"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/tensorflow-ring-animal-detector/src\n"]}],"source":["cd /content/tensorflow-ring-animal-detector/src"]},{"cell_type":"markdown","metadata":{"id":"tCNjfx_Bb7e0"},"source":["Next we want to use the VideoStream class, which allows for frame by frame analysis (with a given framerate) of an mp4 video.\n","\n","This script does the following -\n","\n","\n","* Takes a pre-trained model (PATH_TO_CKPT)\n","* Accesses a label map\n","* Instantiates the TensorFlow interpreter\n","* Goes frame by frame through a video performing motion detection and object detection\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11o7qzQNGc7bU7KE88AwXDq82AGOG3hcb"},"executionInfo":{"elapsed":26497,"status":"error","timestamp":1725964925894,"user":{"displayName":"James Milward","userId":"14091404391319358325"},"user_tz":-60},"id":"1zEFiiBGSXCv","outputId":"a033421c-7bd2-42a9-9ba8-12acc45ad7b5"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from datetime import datetime, timedelta\n","from tensorflow.lite.python.interpreter import Interpreter\n","from video_streaming.video_stream import VideoStream\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# Path to .tflite file, which contains the model that is used for object detection\n","PATH_TO_CKPT = os.path.join('/content/tensorflow-ring-animal-detector/src/models/custom_trained/ssdmobilenet_v2_320/detect.tflite')\n","\n","# Path to label map file\n","PATH_TO_LABELS = os.path.join('/content/tensorflow-ring-animal-detector/training-content/labelmap.txt')\n","\n","# Video Resolutions\n","resolution_w = 640\n","resolution_h = 480\n","\n","# Load the label map\n","with open(PATH_TO_LABELS, 'r') as f:\n","    labels = [line.strip() for line in f.readlines()]\n","\n","# Load the Tensorflow Lite model.\n","interpreter = Interpreter(model_path=PATH_TO_CKPT)\n","interpreter.allocate_tensors()\n","\n","# Get and store/calculate TFLite model details\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","height = input_details[0]['shape'][1]\n","width = input_details[0]['shape'][2]\n","floating_model = (input_details[0]['dtype'] == np.float32)\n","input_mean = 127.5\n","input_std = 127.5\n","boxes_idx, classes_idx, scores_idx = 1, 3, 0 # This is a TF2 model\n","\n","# Initialize frame rate calculation\n","frame_rate_calc = 1\n","freq = cv2.getTickFrequency()\n","\n","# Initialize video stream\n","videostream = VideoStream(\n","    resolution=(int(resolution_w),int(resolution_h)),\n","    video_url_or_filename='/content/tensorflow-ring-animal-detector/src/test-video/badger-stock-video.mp4',\n","    framerate=10,\n","    video_file_analysis=True).start()\n","window_name = \"Animal Object Detection\"\n","\n","# Max streaming time\n","time_to_stop_video = datetime.now() + timedelta(seconds=float(30))\n","\n","# FPS control\n","fps_delay = datetime.now() + timedelta(milliseconds=100)\n","\n","# Motion detection variables\n","first_frame = None\n","frame_diff = None\n","motion_contours = None\n","\n","while True:\n","\n","    # Start timer (for calculating frame rate)\n","    t1 = cv2.getTickCount()\n","\n","    # Implement FPS delay\n","    if datetime.now() >= fps_delay:\n","\n","        try:\n","            # Grab frame from video stream\n","            frame1 = videostream.read()\n","            if type(frame1) is not None:\n","\n","                frame1 = cv2.resize(frame1, (resolution_w, resolution_h))\n","\n","                # Acquire frame and resize to expected shape [1xHxWx3]\n","                frame = frame1.copy()\n","                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frame_resized = cv2.resize(frame_rgb, (width, height))\n","                input_data = np.expand_dims(frame_resized, axis=0)\n","\n","                #### Motion Detection ####\n","                # Thanks to https://towardsdatascience.com/image-analysis-for-beginners-creating-a-motion-detector-with-opencv-4ca6faba4b42\n","                # Prepare image; grayscale and blur\n","                prepared_frame = cv2.cvtColor(frame_rgb, cv2.COLOR_BGR2GRAY)\n","                prepared_frame = cv2.GaussianBlur(src=prepared_frame, ksize=(5,5), sigmaX=0)\n","\n","                # Calculate difference and update previous frame\n","                if (first_frame is None):\n","                    first_frame = prepared_frame\n","                else:\n","                    frame_diff = cv2.absdiff(src1=first_frame, src2=prepared_frame) # calculate the dif\n","                    first_frame = prepared_frame # replace the first frame\n","\n","                    # Dilute the image a bit to make differences more seeable; more suitable for contour detection\n","                    kernel = np.ones((5, 5))\n","                    frame_diff = cv2.dilate(frame_diff, kernel, 1)\n","\n","                    # Only take different areas that are different enough (>10 / 255)\n","                    thresh_frame = cv2.threshold(src=frame_diff, thresh=25, maxval=255, type=cv2.THRESH_BINARY)[1]\n","                    plt.imshow(thresh_frame)\n","                    plt.show()\n","\n","                    # Use motion contours when an object is detected and correlate\n","                    motion_contours, _ = cv2.findContours(image=thresh_frame, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)\n","\n","                #### Object Detection ####\n","                # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","                if floating_model:\n","                    input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","                # Perform the actual detection by running the model with the image as input\n","                interpreter.set_tensor(input_details[0]['index'],input_data)\n","                interpreter.invoke()\n","\n","                # Retrieve detection results\n","                boxes = interpreter.get_tensor(output_details[boxes_idx]['index'])[0] # Bounding box coordinates of detected objects\n","                classes = interpreter.get_tensor(output_details[classes_idx]['index'])[0] # Class indeqx of detected objects\n","                scores = interpreter.get_tensor(output_details[scores_idx]['index'])[0] # Confidence of detected objects\n","\n","                # Loop over all detections and draw detection box if confidence is above minimum threshold (60%)\n","                for i in range(len(scores)):\n","                    if ((scores[i] > 0.6) and (scores[i] <= 1.0)):\n","\n","                        # Get bounding box coordinates and draw box\n","                        # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","                        ymin = int(max(1,(boxes[i][0] * resolution_h)))\n","                        xmin = int(max(1,(boxes[i][1] * resolution_w)))\n","                        ymax = int(min(resolution_h,(boxes[i][2] * resolution_h)))\n","                        xmax = int(min(resolution_w,(boxes[i][3] * resolution_w)))\n","                        cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n","\n","                        # Draw label\n","                        object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","                        label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n","                        labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n","                        label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","                        cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","                        cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n","\n","                        # Motion detection inside object detection - determine if it's near the object that's been detected.\n","                        # This improves object detection as it can mistakenly recognise stationary grass/leaves/patterns as animals\n","                        motion_near_object = False\n","                        search_hit = bool([ele for ele in ['fox', 'badger'] if(ele in object_name)])\n","                        if search_hit:\n","                            if motion_contours != None:\n","                                for contour in motion_contours:\n","                                    (x, y, w, h) = cv2.boundingRect(contour)\n","\n","                                    # Calculate difference between coords of detected object and detected motion\n","                                    ymin_calc = abs(ymin-y)\n","                                    xmin_calc = abs(xmin-x)\n","                                    ymax_calc = abs(ymax-(y+h))\n","                                    xmax_calc = abs(xmax-(x+w))\n","\n","                                    # Motion detected if difference is no lower than threshold (20% of resolution width)\n","                                    threshold = resolution_w/20\n","                                    if ymin_calc <= threshold and xmin_calc <= threshold and ymax_calc <= threshold and xmax_calc <= threshold:\n","                                        cv2.rectangle(img=frame, pt1=(x, y), pt2=(x + w, y + h), color=(255, 0, 0), thickness=2) # blue\n","                                        motion_near_object = True\n","\n","                # Draw framerate in corner of frame\n","                cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),2,cv2.LINE_AA)\n","\n","                # All the results have been drawn on the frame, so it's time to display it.\n","                plt.imshow(frame)\n","                plt.show()\n","\n","                # Calculate framerate\n","                t2 = cv2.getTickCount()\n","                time1 = (t2-t1)/freq\n","                frame_rate_calc= 1/time1\n","\n","                # Reset FPS delay\n","                fps_delay = datetime.now() + timedelta(milliseconds=100)\n","\n","                if time_to_stop_video <= datetime.now():\n","                    break\n","\n","        except Exception as inst:\n","            # Clean up and stop\n","            videostream.stop()\n","\n","# Clean up\n","videostream.stop()"]},{"cell_type":"markdown","metadata":{},"source":["# Finally\n","\n","You should now have seen object detection and motion detection running against the supplied stock footage of a badger! \n","\n","Some notes\n","\n","* The Matlab plt. funtionality was used here instead of cv2.imshow() as the cv2 window functionality for showing images is not supported by Colab\n","* There may have been a few bugs in the frame-by-frame loop - this is a work in progress and I will be fixing this\n","* Further reading on how this works is contained within the tensorflow-ring-animal-detector repo which provides instructions on running this locally in vscode and training your own model"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOEO+HoibMvm0K4M5XKjmUy","gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
